{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7cd5dbf6-c555-46e4-ac6c-2ebf29481e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in c:\\users\\naish\\anaconda3\\lib\\site-packages (0.2.14)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\naish\\anaconda3\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\naish\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: speechrecognition in c:\\users\\naish\\anaconda3\\lib\\site-packages (3.10.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from speechrecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\naish\\anaconda3\\lib\\site-packages (from speechrecognition) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (2024.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\naish\\anaconda3\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyaudio\n",
    "!pip install sounddevice\n",
    "!pip install speechrecognition\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2cf5108b-1034-4e39-9f03-882101b563be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\naish\\anaconda3\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\naish\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\naish\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "#python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "33f9410f-9bf0-4ed9-8b2b-b840e7c1e022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Amount              Item\n",
      "0       50           bananas\n",
      "1      100         Starbucks\n",
      "2      200  electricity bill\n",
      "3      150              fare\n",
      "4      300            ticket\n",
      "..     ...               ...\n",
      "239     20              card\n",
      "240    300             scarf\n",
      "241     80             drink\n",
      "242    700      fitness band\n",
      "243     50             snack\n",
      "\n",
      "[244 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Sample transaction data\n",
    "'''transactions = [\n",
    "    \"Bought 50rs of bananas\",\n",
    "    \"Rs. 100 at Starbucks\",\n",
    "    \"Paid 200rs for electricity bill\",\n",
    "    \"Taxi fare of 150rs\",\n",
    "    \"Movie ticket for 300rs\",\n",
    "    \"Paid 75 bucks at the cafe\",\n",
    "    \"Grocery shopping for 120 rupees\",\n",
    "    \"Spent 500 rs on a new phone\",\n",
    "    \"Cost 25 rupee for the book\",\n",
    "    \"Spent 50.5 rs on fruits\",\n",
    "    \"I spent Rupees 100 on water\",\n",
    "    \"I spent 200 rupees on groceries\",\n",
    "    \"I bought a movie ticket for 500 rupees\",\n",
    "    \"Spent 300 on bus fare\",\n",
    "    \"Paid 1000 rupees for a doctor visit\",\n",
    "    \"Bought new furniture for 1500 rupees\",\n",
    "    \"Spent 600 on books\",\n",
    "    \"Paid 700 for electricity bill\",\n",
    "    \"Spent 1200 rupees on a concert\",\n",
    "    \"I spent 250 on fruits\",\n",
    "    \"Paid 400 for a taxi ride\",\n",
    "    \"I visited the doctor and spent 1000rs\"\n",
    "]'''\n",
    "\n",
    "transactions=[\"Bought 50rs of bananas\", \"Rs. 100 at Starbucks\", \"Paid 200rs for electricity bill\", \"Taxi fare of 150rs\", \"Movie ticket for 300rs\", \"Paid 75 bucks at the cafe\", \"Grocery shopping for 120 rupees\", \"Spent 500 rs on a new phone\", \"Cost 25 rupee for the book\", \"Spent 50.5 rs on fruits\", \"I spent Rupees 100 on water\", \"I spent 200 rupees on groceries\", \"I bought a movie ticket for 500 rupees\", \"Spent 300 on bus fare\", \"Paid 1000 rupees for a doctor visit\", \"Bought new furniture for 1500 rupees\", \"Spent 600 on books\", \"Paid 700 for electricity bill\", \"Spent 1200 rupees on a concert\", \"I spent 250 on fruits\", \"Paid 400 for a taxi ride\", \"I visited the doctor and spent 1000rs\", \"Spent 220 rupees on transportation\", \"Spent 230 rupees on entertainment\", \"Spent 240 rupees on household\", \"Spent 250 rupees on shopping\", \"Spent 260 rupees on health\", \"Spent 270 rupees on education\", \"Spent 280 rupees on gift\", \"Spent 290 rupees on others\", \"Spent 300 rupees on food\", \"Spent 310 rupees on social_life\", \"Spent 3460 rupees on health\", \"Spent 3470 rupees on education\", \"Spent 3480 rupees on gift\", \"Spent 3490 rupees on others\", \"Spent 450 rupees on dinner with friends.\", \"Bought a souvenir for 200 rupees.\", \"Paid 75 rs for a postcard.\", \"Spent 300 rupees on a photo frame.\", \"Bought a wallet for 500 rupees.\", \"Spent 120 rupees on face wash.\", \"Paid 40 rs for a bookmark.\", \"Bought a cup of coffee for 150 rupees.\", \"Paid 350 rupees for a doctor's appointment.\", \"I spent 500 on a nice lunch.\", \"Bought new headphones for 1000 rupees.\", \"Cost 75 rupees for the magazine.\", \"Paid 200 for parking fees.\", \"Spent 60 rupees on a bottle of water.\", \"Taxi fare of 200 rupees.\", \"Bought groceries worth 500 rupees.\", \"Paid 100 rupees for movie snacks.\", \"Spent 120 rupees on stationery.\", \"Paid 300 rs for a haircut.\", \"Spent 200 rupees on a train ticket.\", \"Bought snacks for 100 rupees.\", \"Paid 75 rs for coffee.\", \"Spent 50 rupees on a juice.\", \"Paid 600 rs for a fitness class.\", \"Bought clothes worth 1000 rupees.\", \"Spent 200 rupees on a bus fare.\", \"Cost 150 rupees at the bakery.\", \"Spent 40 rupees on gum.\", \"Paid 1200 rs for the dentist.\", \"Bought a new bag for 700 rupees.\", \"Spent 100 rs on phone recharge.\", \"Paid 300 rupees for flowers.\", \"Spent 500 rs on a family dinner.\", \"Paid 250 for a museum ticket.\", \"Bought books worth 350 rupees.\", \"Spent 80 rupees on tea.\", \"Cost 900 rupees at the optician.\", \"Paid 1000 rs for sports equipment.\", \"Spent 500 rupees on clothes.\", \"Taxi fare of 350 rs.\", \"Spent 50 rupees on stationery.\", \"Paid 60 rs for an ice cream.\", \"Spent 2000 rupees on a new pair of shoes.\", \"Bought a cake for 500 rupees.\", \"Paid 1000 rs for the hotel stay.\", \"Spent 150 rupees on a meal.\", \"Bought tickets worth 800 rupees.\", \"Spent 350 on a concert ticket.\", \"Paid 400 rupees for medical supplies.\", \"Spent 100 rs on a movie rental.\", \"Cost 75 rs for entry fees.\", \"Bought a hat for 250 rupees.\", \"Spent 180 rupees on phone accessories.\", \"Paid 500 rs for a cooking class.\", \"Spent 120 rupees on a gift.\", \"Bought fruit worth 75 rupees.\", \"Spent 650 rs on a dress.\", \"Paid 2500 rupees on an online course.\", \"Spent 300 rupees on lunch.\", \"Taxi fare of 100 rs.\", \"Spent 200 rupees on a pedicure.\", \"Paid 350 rs for a game.\", \"Bought groceries for 250 rupees.\", \"Paid 125 rs at the coffee shop.\", \"Spent 90 rupees on a snack.\", \"Bought veggies worth 200 rupees.\", \"Spent 1200 rs on a fitness tracker.\", \"Paid 45 rs for a candy bar.\", \"Taxi fare of 150 rs.\", \"Spent 300 rupees on groceries.\", \"Bought a magazine for 50 rs.\", \"Paid 75 rupees for the library fee.\", \"Spent 250 rupees on a movie ticket.\", \"Spent 100 rupees on a cold drink.\", \"Paid 700 rupees for car service.\", \"Bought accessories for 400 rs.\", \"Spent 60 rupees on popcorn.\", \"Paid 500 rupees for a phone case.\", \"Bought a t-shirt for 300 rupees.\", \"Paid 80 rs for a notebook.\", \"Spent 150 rupees on stationery.\", \"Paid 450 rs at the dentist.\", \"Bought flowers worth 100 rupees.\", \"Spent 1200 rupees on groceries.\", \"Taxi fare of 90 rs.\", \"Spent 200 rs on a cab ride.\", \"Paid 120 rupees at the coffee shop.\", \"Bought a scarf for 200 rupees.\", \"Spent 75 rs on street food.\", \"Paid 300 rupees for gym membership.\", \"Spent 2500 rs on a hotel room.\", \"Bought an umbrella for 150 rupees.\", \"Spent 100 rs on a drink.\", \"Paid 500 rs for a city tour.\", \"Bought a novel for 350 rupees.\", \"Spent 120 rupees on juice.\", \"Paid 400 rupees for car wash.\", \"Bought a jacket worth 800 rs.\", \"Spent 150 rupees on a meal.\", \"Paid 90 rupees for a newspaper.\", \"Spent 200 rupees on dinner.\", \"Bought a bag for 1000 rupees.\", \"Paid 70 rupees for a bookmark.\", \"Spent 350 rs on laundry.\", \"Bought cosmetics for 200 rupees.\", \"Paid 50 rupees for a pen.\", \"Spent 1200 rupees on a concert ticket.\", \"Spent 180 rupees on stationery.\", \"Bought a sandwich for 50 rupees\", \"Paid 200 rupees for gym fees\", \"Spent 150 rupees on a coffee\", \"Bought a book for 300 rupees\", \"Paid 100 rupees for snacks\", \"Spent 400 rupees on a shirt\", \"Taxi fare of 120 rupees\", \"Bought a pen for 20 rupees\", \"Spent 500 rupees on a birthday gift\", \"Paid 60 rupees for a bottle of water\", \"Spent 150 rupees on dinner\", \"Bought groceries for 700 rupees\", \"Spent 80 rupees on a magazine\", \"Paid 1000 rupees for a health check-up\", \"Bought a concert ticket for 900 rupees\", \"Spent 50 rupees on stationery\", \"Paid 75 rupees for coffee at Starbucks\", \"Spent 60 rupees on a snack\", \"Bought a scarf for 200 rupees\", \"Paid 400 rupees for lunch with friends\", \"Spent 30 rupees on gum\", \"Bought a new water bottle for 100 rupees\", \"Spent 350 rupees on school supplies\", \"Paid 300 rupees for a taxi ride\", \"Bought a phone case for 150 rupees\", \"Spent 200 rupees on a bus ticket\", \"Paid 80 rupees for a notepad\", \"Bought a pack of biscuits for 50 rupees\", \"Spent 120 rupees on movie snacks\", \"Paid 600 rupees for a cooking class\", \"Spent 400 rupees on a family meal\", \"Bought a new wallet for 250 rupees\", \"Paid 45 rupees for a soda\", \"Spent 350 rupees on a museum ticket\", \"Bought a fruit basket for 150 rupees\", \"Spent 220 rupees on a taxi ride\", \"Paid 75 rupees for an ice cream\", \"Spent 100 rupees on chocolates\", \"Bought a novel for 250 rupees\", \"Paid 2000 rupees for a weekend stay\", \"Spent 90 rupees on a fruit juice\", \"Bought groceries for 600 rupees\", \"Spent 800 rupees on a new shirt\", \"Paid 70 rupees for a tea\", \"Bought a lunchbox for 200 rupees\", \"Spent 50 rupees on a cold drink\", \"Paid 400 rupees for flowers\", \"Bought a pair of sunglasses for 300 rupees\", \"Spent 75 rupees on parking fees\", \"Paid 100 rupees for movie rental\", \"Bought a coffee for 120 rupees\", \"Spent 600 rupees on groceries\", \"Paid 300 rupees for car wash\", \"Bought a cake for 250 rupees\", \"Spent 400 rupees on a fitness tracker\", \"Paid 1500 rupees for gym membership\", \"Bought a novel for 350 rupees\", \"Spent 100 rupees on a drink\", \"Paid 50 rupees for candy\", \"Bought a jacket for 700 rupees\", \"Spent 500 rupees on a family dinner\", \n",
    "\"Paid 120 rupees for street food\", \"Bought cosmetics for 300 rupees\", \"Spent 150 rupees on a juice\", \"Paid 700 rupees for doctorâ€™s appointment\", \"Bought a keychain for 50 rupees\", \"Spent 60 rupees on popcorn\", \"Paid 300 rupees for a movie\", \"Bought a new pair of shoes for 1200 rupees\", \"Spent 500 rupees on groceries\", \"Paid 100 rupees for a bus pass\", \"Bought a souvenir for 100 rupees\", \"Spent 200 rupees on dinner with friends\", \"Paid 90 rupees for a newspaper\", \"Bought a bottle of water for 20 rupees\", \"Spent 300 rupees on phone accessories\", \"Paid 600 rupees for a cooking course\", \"Bought a birthday gift for 400 rupees\", \"Spent 500 rupees on a backpack\", \"Paid 200 rupees for a train ticket\", \"Bought a magazine for 60 rupees\", \"Spent 40 rupees on stationery\", \"Paid 100 rupees for snacks\", \"Bought groceries worth 750 rupees\", \"Spent 1500 rupees on a weekend getaway\", \"Paid 250 rupees for entry fees\", \"Bought an umbrella for 300 rupees\", \"Spent 1000 rupees on a gym subscription\", \"Paid 30 rupees for a bookmark\", \"Bought a new bag for 900 rupees\", \"Spent 600 rupees on clothes\", \"Paid 200 rupees for a manicure\", \"Bought a movie ticket for 400 rupees\", \"Spent 60 rupees on coffee\", \"Paid 150 rupees for a magazine\", \"Bought a birthday card for 20 rupees\", \"Spent 300 rupees on a scarf\", \"Paid 80 rupees for a cold drink\", \"Bought a fitness band for 700 rupees\", \"Spent 50 rupees on a snack\" ]\n",
    "\n",
    "# Define the improved extraction function\n",
    "def extract_amount_item(transaction):\n",
    "    # Tokenize the transaction text\n",
    "    tokens = nltk.word_tokenize(transaction)\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Compile a list of common currency words to exclude from item extraction\n",
    "    currency_terms = {\"rs\", \"rupees\", \"bucks\", \"dollars\", \"pounds\", \"cost\",\"rupee\", \"lakh\", \"crore\", \"million\"}\n",
    "    \n",
    "    # Extract the amount using a regex pattern and only keep the numeric part\n",
    "    amount_pattern = re.compile(r'\\b(\\d+(?:\\.\\d+)?)\\s*(?:rs|rupees|bucks|dollars|pounds)?', re.IGNORECASE)\n",
    "    amount_match = amount_pattern.search(transaction)\n",
    "    amount = amount_match.group(1) if amount_match else \"Unknown\"\n",
    "    \n",
    "    # Extract nouns as potential items, filtering out currency terms\n",
    "    item_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Collect nouns (NN, NNS) that are not currency terms\n",
    "        if tag in ['NN', 'NNS'] and word.lower() not in currency_terms:\n",
    "            item_tokens.append(word)\n",
    "    \n",
    "    # Join tokens to form the item name, if available\n",
    "    item = \" \".join(item_tokens) if item_tokens else \"Unknown\"\n",
    "    \n",
    "    return amount.strip(), item.strip()\n",
    "\n",
    "# Apply the refined extraction to all transactions\n",
    "extracted_data = [extract_amount_item(transaction) for transaction in transactions]\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(extracted_data, columns=['Amount', 'Item'])\n",
    "\n",
    "# Print the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ccb4a5bb-544f-4662-baa9-eb0e97c479b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\naish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9e81e7ea-81c8-487b-8282-4de05b6b5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: fries, Category: others\n",
      "Item: taxi, Category: transportation\n",
      "Item: concert, Category: social_life\n",
      "Item: jacket, Category: shopping\n",
      "Item: phone, Category: entertainment\n",
      "Item: sofa, Category: household\n",
      "Item: medicine, Category: health\n",
      "Item: shampoo, Category: others\n",
      "Item: textbook, Category: education\n",
      "Item: beer, Category: food\n",
      "Item: electricity, Category: household\n",
      "Item: movie, Category: social_life\n",
      "Item: yoga, Category: entertainment\n",
      "Item: coffee, Category: food\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Define expanded categories with additional relevant synsets in WordNet\n",
    "hypernym_categories = {\n",
    "    'food': [\n",
    "        wn.synset('food.n.01'), wn.synset('meal.n.01'), wn.synset('snack.n.01'), wn.synset('drink.n.01'),\n",
    "        wn.synset('dish.n.01'), wn.synset('fruit.n.01'), wn.synset('vegetable.n.01'), wn.synset('dessert.n.01'),\n",
    "        wn.synset('grocery.n.01'), wn.synset('produce.n.01'), wn.synset('beverage.n.01'), wn.synset('cooking.n.01'),\n",
    "        wn.synset('eating.n.01'), wn.synset('cuisine.n.01'), wn.synset('dairy_product.n.01'), wn.synset('meat.n.01'),\n",
    "        wn.synset('seafood.n.01'), wn.synset('spice.n.01'), wn.synset('ingredient.n.01')\n",
    "    ],\n",
    "    'social_life': [\n",
    "        wn.synset('recreation.n.01'), wn.synset('celebration.n.01'), wn.synset('outing.n.01'), wn.synset('party.n.01'),\n",
    "        wn.synset('dancing.n.01'), wn.synset('social_event.n.01'), wn.synset('entertainment.n.01'), wn.synset('concert.n.01'),\n",
    "        wn.synset('movie.n.01'), wn.synset('gathering.n.01'), wn.synset('festival.n.01'), wn.synset('show.n.01'),\n",
    "        wn.synset('nightclub.n.01'), wn.synset('bar.n.01'), wn.synset('pub.n.01'), wn.synset('celebration.n.01')\n",
    "    ],\n",
    "    'transportation': [\n",
    "        wn.synset('vehicle.n.01'), wn.synset('transportation.n.01'), wn.synset('public_transport.n.01'), wn.synset('taxi.n.01'),\n",
    "        wn.synset('airplane.n.01'), wn.synset('car.n.01'), wn.synset('train.n.01'), wn.synset('bus.n.01'),\n",
    "        wn.synset('fare.n.01'), wn.synset('subway.n.01'), wn.synset('railway.n.01'), wn.synset('transport.n.01'),\n",
    "        wn.synset('commute.n.01'), wn.synset('travel.n.01'), wn.synset('motor_vehicle.n.01'), wn.synset('bicycle.n.01'),\n",
    "        wn.synset('ticket.n.01'), wn.synset('road.n.01')\n",
    "    ],\n",
    "    'entertainment': [\n",
    "        wn.synset('culture.n.01'), wn.synset('art.n.01'), wn.synset('entertainment.n.01'), wn.synset('concert.n.01'),\n",
    "        wn.synset('museum.n.01'), wn.synset('movie.n.01'), wn.synset('game.n.01'), wn.synset('sport.n.01'),\n",
    "        wn.synset('amusement.n.01'), wn.synset('theater.n.01'), wn.synset('exhibition.n.01'), wn.synset('show.n.01'),\n",
    "        wn.synset('play.n.01'), wn.synset('performance.n.01'), wn.synset('event.n.01'), wn.synset('hobby.n.01')\n",
    "    ],\n",
    "    'household': [\n",
    "        wn.synset('household.n.01'), wn.synset('furniture.n.01'), wn.synset('appliance.n.01'), wn.synset('utility.n.01'),\n",
    "        wn.synset('cleaning.n.01'), wn.synset('kitchenware.n.01'), wn.synset('decoration.n.01'), wn.synset('bedding.n.01'),\n",
    "        wn.synset('laundry.n.01'), wn.synset('repair.n.01'), wn.synset('gardening.n.01'), wn.synset('maintenance.n.01'),\n",
    "        wn.synset('utensil.n.01'), wn.synset('fixture.n.01'), wn.synset('housework.n.01')\n",
    "    ],\n",
    "    'shopping': [\n",
    "        wn.synset('clothing.n.01'), wn.synset('footwear.n.01'), wn.synset('accessory.n.01'), wn.synset('outerwear.n.01'),\n",
    "        wn.synset('toiletry.n.01'), wn.synset('cosmetic.n.01'), wn.synset('jewelry.n.01'), wn.synset('apparel.n.01'),\n",
    "        wn.synset('bag.n.01'), wn.synset('watch.n.01'), wn.synset('fashion.n.01'), wn.synset('retail.n.01'),\n",
    "        wn.synset('boutique.n.01'), wn.synset('department_store.n.01'), wn.synset('shopping.n.01')\n",
    "    ],\n",
    "    'health': [\n",
    "        wn.synset('health.n.01'), wn.synset('medicine.n.01'), wn.synset('therapy.n.01'), wn.synset('fitness.n.01'),\n",
    "        wn.synset('exercise.n.01'), wn.synset('training.n.01'), wn.synset('meditation.n.01'), wn.synset('nutrition.n.01'),\n",
    "        wn.synset('doctor.n.01'), wn.synset('hospital.n.01'), wn.synset('wellness.n.01'), wn.synset('pharmacy.n.01'),\n",
    "        wn.synset('clinic.n.01'), wn.synset('dentist.n.01'), wn.synset('treatment.n.01'), wn.synset('nurse.n.01'),\n",
    "        wn.synset('diet.n.01')\n",
    "    ],\n",
    "    'education': [\n",
    "        wn.synset('education.n.01'), wn.synset('schooling.n.01'), wn.synset('book.n.01'), wn.synset('course.n.01'),\n",
    "        wn.synset('lecture.n.01'), wn.synset('research.n.01'), wn.synset('workshop.n.01'), wn.synset('library.n.01'),\n",
    "        wn.synset('stationery.n.01'), wn.synset('tuition.n.01'), wn.synset('student.n.01'), wn.synset('class.n.01'),\n",
    "        wn.synset('teacher.n.01'), wn.synset('exam.n.01'), wn.synset('assignment.n.01'), wn.synset('university.n.01')\n",
    "    ],\n",
    "    'gift': [\n",
    "        wn.synset('gift.n.01'), wn.synset('present.n.01'), wn.synset('souvenir.n.01'), wn.synset('donation.n.01'),\n",
    "        wn.synset('offering.n.01'), wn.synset('keepsake.n.01'), wn.synset('award.n.01'), wn.synset('memento.n.01'),\n",
    "        wn.synset('prize.n.01'), wn.synset('trophy.n.01'), wn.synset('celebration.n.01'), wn.synset('token.n.01')\n",
    "    ],\n",
    "    'others': [\n",
    "        wn.synset('artifact.n.01'), wn.synset('object.n.01'), wn.synset('thing.n.01'), wn.synset('item.n.01'),\n",
    "        wn.synset('material.n.01'), wn.synset('equipment.n.01'), wn.synset('supply.n.01'), wn.synset('instrumentality.n.01'),\n",
    "        wn.synset('device.n.01'), wn.synset('commodity.n.01'), wn.synset('resource.n.01'), wn.synset('tool.n.01')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten synsets for each category to enable direct comparison\n",
    "flattened_hypernym_categories = {cat: synsets for cat, synsets in hypernym_categories.items()}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "direct_mappings = {\n",
    "    'bananas': 'food', 'starbucks': 'food', 'electricity bill': 'household', 'fare': 'transportation', \n",
    "    'ticket': 'entertainment', 'cafe': 'food', 'shopping': 'shopping', 'phone': 'entertainment', \n",
    "    'rupee book': 'education', 'water': 'food', 'groceries': 'food', 'movie ticket': 'entertainment',\n",
    "    'bus fare': 'transportation', 'doctor visit': 'health', 'furniture': 'household', 'books': 'education', \n",
    "    'electricity': 'household', 'concert': 'social_life', 'taxi ride': 'transportation'\n",
    "}\n",
    "\n",
    "# Helper function to check if any synset in a hypernym path matches the target synsets for a category\n",
    "def is_hypernym_in_path(synset, target_hypernyms):\n",
    "    hypernym_paths = synset.hypernym_paths()  # Get all hypernym paths\n",
    "    for path in hypernym_paths:\n",
    "        for ancestor in path:\n",
    "            if ancestor in target_hypernyms:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Get category by hypernym path and semantic similarity\n",
    "def get_category(item):\n",
    "    # Take only the first word if the item has multiple words\n",
    "    first_word = item.split()[0]\n",
    "    \n",
    "    # Normalize and lemmatize the first word\n",
    "    lemmatized_item = lemmatizer.lemmatize(first_word.lower())\n",
    "    \n",
    "    # Check direct mappings first\n",
    "    if lemmatized_item in direct_mappings:\n",
    "        return direct_mappings[lemmatized_item]\n",
    "    \n",
    "    # Get synsets for the item as a noun\n",
    "    item_synsets = wn.synsets(lemmatized_item, pos=wn.NOUN)\n",
    "    for synset in item_synsets:\n",
    "        # Check if any synset in the item's hypernym path matches a category\n",
    "        for category, target_hypernyms in hypernym_categories.items():\n",
    "            if is_hypernym_in_path(synset, target_hypernyms):\n",
    "                return category\n",
    "\n",
    "    # As a last resort, use semantic similarity to find the closest category\n",
    "    best_category = 'others'\n",
    "    max_similarity = 0\n",
    "    for synset in item_synsets:\n",
    "        for category, target_hypernyms in hypernym_categories.items():\n",
    "            for target_synset in target_hypernyms:\n",
    "                similarity = synset.path_similarity(target_synset)\n",
    "                if similarity and similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_category = category\n",
    "    return best_category\n",
    "\n",
    "\n",
    "# Sample dataset\n",
    "spending_data = [\n",
    "    {'item': 'fries', 'amount': 100},\n",
    "    {'item': 'taxi', 'amount': 50},\n",
    "    {'item': 'concert', 'amount': 200},\n",
    "    {'item': 'jacket', 'amount': 150},\n",
    "    {'item': 'phone', 'amount': 300},\n",
    "    {'item': 'sofa', 'amount': 500},\n",
    "    {'item': 'medicine', 'amount': 100},\n",
    "    {'item': 'shampoo', 'amount': 30},\n",
    "    {'item': 'textbook', 'amount': 200},\n",
    "    {'item': 'beer', 'amount': 60},\n",
    "    {'item': 'electricity', 'amount': 80},\n",
    "    {'item': 'movie', 'amount': 1200},\n",
    "    {'item': 'yoga', 'amount': 300},\n",
    "    {'item': 'coffee', 'amount': 50},\n",
    "]\n",
    "\n",
    "# Categorize each item\n",
    "for entry in spending_data:\n",
    "    item = entry['item']\n",
    "    category = get_category(item)\n",
    "    print(f\"Item: {item}, Category: {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "79fc86b3-a0ea-4373-9800-770e5c087685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'] = df['Item'].apply(get_category)\n",
    "df\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('path_to_your_file2.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a45bf6d8-34f2-40c3-bcf1-3c5a74d82d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "RATE = 16000  # Sample rate\n",
    "CHUNK = 1024  # Number of frames per buffer\n",
    "\n",
    "def is_silent(data, threshold=500):\n",
    "    \"\"\"Returns 'True' if below the 'silent' threshold\"\"\"\n",
    "    return np.abs(np.frombuffer(data, dtype=np.int16)).max() < threshold\n",
    "\n",
    "def record_audio():\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    audio_data = io.BytesIO()\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Listening...\")\n",
    "    silence_start = None\n",
    "    audio_chunks = []\n",
    "    \n",
    "    while True:\n",
    "        data = stream.read(CHUNK)\n",
    "        audio_chunks.append(data)\n",
    "        \n",
    "        if is_silent(data):\n",
    "            if silence_start is None:\n",
    "                silence_start = time.time()\n",
    "            elif time.time() - silence_start > 3:\n",
    "                break\n",
    "        else:\n",
    "            silence_start = None\n",
    "    \n",
    "    print(\"Recording complete\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    audio_data.write(b''.join(audio_chunks))\n",
    "    audio_data.seek(0)\n",
    "    return audio_data, recognizer\n",
    "\n",
    "def speech_to_text(audio_data, recognizer):\n",
    "    audio = sr.AudioData(audio_data.read(), RATE, 2)\n",
    "    text = recognizer.recognize_google(audio)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "35e60cab-8364-4d80-b453-cb22c81e5c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Recording complete\n",
      "Recognized Text: I got medicines worth Rs 300\n",
      "   Amount                 Item        Category                  Timestamp\n",
      "0      50              bananas          others                        NaT\n",
      "1     100            Starbucks            food                        NaT\n",
      "2     200     electricity bill       household                        NaT\n",
      "3     150                 fare  transportation                        NaT\n",
      "4     300               ticket   entertainment                        NaT\n",
      "5      75                 cafe            food                        NaT\n",
      "6     120             shopping        shopping                        NaT\n",
      "7     500                phone   entertainment                        NaT\n",
      "8      25                 book       education                        NaT\n",
      "9    50.5               fruits            food                        NaT\n",
      "10    100                water            food                        NaT\n",
      "11    200            groceries            food                        NaT\n",
      "12    500         movie ticket     social_life                        NaT\n",
      "13    300             bus fare  transportation                        NaT\n",
      "14   1000         doctor visit          health                        NaT\n",
      "15   1500            furniture       household                        NaT\n",
      "16    600                books       education                        NaT\n",
      "17    700     electricity bill       household                        NaT\n",
      "18   1200              concert     social_life                        NaT\n",
      "19    250               fruits            food                        NaT\n",
      "20    400            taxi ride  transportation                        NaT\n",
      "21   1000               doctor          health                        NaT\n",
      "22    100  strawberry hospital            food 2024-11-06 20:06:39.165501\n",
      "23    300            medicines          health 2024-11-06 20:49:43.968362\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "audio_data, recognizer = record_audio()\n",
    "    \n",
    "# Convert speech to text\n",
    "text = speech_to_text(audio_data, recognizer)\n",
    "print(\"Recognized Text:\", text)\n",
    "amt, item = extract_amount_item(text)\n",
    "category = get_category(item)\n",
    "timestamp = datetime.now()\n",
    "\n",
    "    # Create a new DataFrame for the new row\n",
    "new_row_df = pd.DataFrame([[amt, item, category,timestamp]], columns=['Amount', 'Item', 'Category', 'Timestamp'])\n",
    "\n",
    "    # Concatenate the new row to the existing DataFrame\n",
    "df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "\n",
    "    # Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774faa6b-5fb3-497e-9ba1-eeac2a83c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8ecf4-d343-41ec-85cf-d867882bf7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
